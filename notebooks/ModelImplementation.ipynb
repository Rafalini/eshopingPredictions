{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from models import build_model, compare_result\n",
    "\n",
    "sessions = pd.read_csv(\"merged_dataset\", sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506df787",
   "metadata": {},
   "source": [
    "## Usunięcie redundantnych atrybutów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b281ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = sessions.drop(['session_id', 'user_id', 'unique_categories'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf074804",
   "metadata": {},
   "source": [
    "## One-hot encoding dla danych zawierających labele, a nie wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.get_dummies(sessions, columns = ['offered_discount', 'weekday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44957b",
   "metadata": {},
   "source": [
    "## Podział zbioru na podzbiory: testowy i treningowy\n",
    "\n",
    "12% danych testowych i 88% danych treningowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c50a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 116\n",
    "test_size = 0.12\n",
    "\n",
    "X = sessions.drop('purchase', axis=1)\n",
    "Y = sessions[['purchase']]\n",
    "records_num = len(sessions)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "print('Size of entire dataset: ', records_num)\n",
    "print('Size of training set: ', len(X_train))\n",
    "print('Size of test set: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c57b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21ed40",
   "metadata": {},
   "source": [
    "## Usunięcie powiązanych ze sobą atrybutów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08af076",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "plt.rcParams.update({'font.size': 32})\n",
    "\n",
    "train = pd.concat([X_train, Y_train], axis=1)\n",
    "\n",
    "sns.heatmap(\n",
    "    train.corr(method = 'spearman'),\n",
    "    xticklabels = train.columns,\n",
    "    yticklabels = train.columns,\n",
    "    annot=True,\n",
    "    fmt='0.1g'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b2ef8",
   "metadata": {},
   "source": [
    "//Wnioski o korelacji rangowej spearmana i jeżeli wyjdą skorelowane atrybuty to musimy zrobić chi kwadrat by zobaczyć, który trzeba usunąć"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ccad6",
   "metadata": {},
   "source": [
    "## Wykonanie testu chi kwadrat na wszystkich atrybutach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4828567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_one =[]\n",
    "feature_ranking = SelectKBest(chi2, k=5)\n",
    "fit = feature_ranking.fit(X_train, Y_train)\n",
    "\n",
    "for i, (score, feature) in enumerate(zip(feature_ranking.scores_, X_train.columns)):\n",
    "    list_one.append((score, feature))\n",
    "    \n",
    "dfObj = pd.DataFrame(list_one) \n",
    "dfObj = dfObj.sort_values(by=[0], ascending = False)\n",
    "\n",
    "display(dfObj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6c89a",
   "metadata": {},
   "source": [
    "//usuwamy skorelowane atrybuty i te bliskie zeru"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5033cbe",
   "metadata": {},
   "source": [
    "## Prezentacja ostatecznych wejść i wyjść konstruowanych modelów\n",
    "\n",
    "VVVV - trzeba przeredagować\n",
    "\n",
    "Zatem naszymi danymi wejściowymi modelu będą następujące atrybuty:\n",
    "\n",
    "duration - długość trwania sesji w sekundach\n",
    "click_rate - liczba zdarzeń(event'ów) do aktualnego rekordu na minutę\n",
    "weekday_0.0 - weekday_6.0 - one hot encoding dni tygodnia\n",
    "hour - godzina\n",
    "offered_discount_10, 15, 20 - one hot encoding 10, 15 oraz 20%-owej zniżki\n",
    "weekend - wartość boolowska odpowiadająca na pytanie czy sesja trwa w weekend\n",
    "Ponadto dodaliśmy nowe atrybuty:\n",
    "\n",
    "last_session_purchase - odpowiada na pytanie czy ostatnia sesja zakończyła się zakupem\n",
    "price - cena przeglądanego produktu\n",
    "Natomiast zmienną celu jest prawdopoboieństwo tego, że sesja zakończy się zakupem. Model będzie zwracał, że sesja zakończy się zakupem jeśli prawdopodobieństwo zakupu będzie większe od pewnej wartości granicznej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa584d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = pd.read_csv(\"merged_dataset\", sep=' ')\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "dropped_cols_with_corelated_attr = list(dfObj[dfObj[0] < 1][1])\n",
    "dropped_cols_with_corelated_attr.extend(['session_id', 'user_id', 'unique_categories'])\n",
    "\n",
    "dropped_cols_without_corelated_attr = dropped_cols_with_corelated_attr[:]\n",
    "dropped_cols_without_corelated_attr.extend(['unique_item_views', 'item_views'])\n",
    "\n",
    "onehot_cols = ['offered_discount', 'weekday']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577cc2a9",
   "metadata": {},
   "source": [
    "## Model bazowy - regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_fn_tuning(X_train, Y_train):\n",
    "    # hyperparameters tuning\n",
    "    model = LogisticRegression(max_iter=500)\n",
    "    # define evaluation\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "\n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "    space['penalty'] = ['l2']\n",
    "    space['C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "    search = RandomizedSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv, random_state=seed)\n",
    "    search.fit(X_train, Y_train)\n",
    "    \n",
    "    print(search.best_estimator_) # model = LogisticRegression(C=0.1, max_iter=500, solver='newton-cg')\n",
    "    return search.best_estimator_\n",
    "\n",
    "def logistic_fn(X_train, Y_train):\n",
    "    model = LogisticRegression(C=0.1, max_iter=500, solver='newton-cg')\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return model\n",
    "\n",
    "result_dict['Logistic Regression with corelated attributes'] = \\\n",
    "    build_model(logistic_fn, sessions, onehot_cols, dropped_cols_with_corelated_attr, 'purchase', \n",
    "                seed, test_size)\n",
    "\n",
    "result_dict['Logistic Regression without corelated attributes'] = \\\n",
    "    build_model(logistic_fn, sessions, onehot_cols, dropped_cols_without_corelated_attr, 'purchase', seed, test_size, output = './microservice/models/logistic_reg.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1cc07b",
   "metadata": {},
   "source": [
    "## Zaawansowany model docelowy - regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498de28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_fn_tuning(X_train,Y_train):\n",
    "    model = RandomForestClassifier()\n",
    "    \n",
    "    # define evaluation\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "    \n",
    "    # define search space\n",
    "    space = dict()\n",
    "    space['n_estimators'] = [100, 200, 250, 300, 350, 400, 500, 800, 1200]\n",
    "    space['max_depth'] = [5, 8, 10, 15, 20, 25, 30]\n",
    "    space['min_samples_split'] = [2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 100]\n",
    "    space['min_samples_leaf'] = [1, 2, 5, 10] \n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "    search = RandomizedSearchCV(model, space, n_iter=50, scoring='accuracy', n_jobs=-1, cv=cv, random_state=seed)\n",
    "    search.fit(X_train, Y_train)\n",
    "    \n",
    "    print(search.best_estimator_) # RandomForestClassifier(max_depth=25, min_samples_split=15, n_estimators=300)\n",
    "    \n",
    "    return search.best_estimator_\n",
    "\n",
    "def random_forest_fn(X_train,Y_train):\n",
    "    model = RandomForestClassifier(max_depth=9, min_samples_split=15, n_estimators=300) \n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "result_dict['Random_Forest with corelated attributes'] = \\\n",
    "    build_model(random_forest_fn, sessions, onehot_cols, dropped_cols_with_corelated_attr,\n",
    "                'purchase', seed)\n",
    "result_dict['Random_Forest without corelated attributes'] = \\\n",
    "    build_model(random_forest_fn, sessions, onehot_cols, dropped_cols_without_corelated_attr, 'purchase', seed, output='./microservice/models/random_forest.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d0e12",
   "metadata": {},
   "source": [
    "## Porównanie wyników prezentowanych przez modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = ['training', 'test']\n",
    "records = []\n",
    "for name in result_dict:\n",
    "    for option in scope:\n",
    "        record = []\n",
    "        for key in result_dict[name][option]:\n",
    "            record.append(result_dict[name][option][key])\n",
    "        records.append(record)\n",
    "            \n",
    "df = pd.DataFrame.from_records(records)\n",
    "df.columns = ['Accuracy', 'Precision', 'Recall', 'F1_score']\n",
    "df.index = ['LR with corelated attributes / training data', 'LR with corelated attributes / test data', \n",
    "            'LR without corelated attributes / training data', 'LR without corelated attributes / test data', \n",
    "            'RF with corelated attributes / training data', 'RF with corelated attributes / test data', \n",
    "            'RF without corelated attributes / training data', 'RF without corelated attributes / test data']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72510ac",
   "metadata": {},
   "source": [
    "## Analiza uzyskanych wyników\n",
    "\n",
    "VVVV - przeredagować \n",
    "\n",
    "\n",
    "Zaproponowaliśmy 2 modele tj. model bazowy będący regresją logistyczną oraz bardziej zaawansowany model docelowy będący modelem lasów losowych. Dodatkowo dla każdego z modeli rozpatrzyliśmy 2 przypadki:\n",
    "\n",
    "uwzględnienie skorelowanych atrybutów\n",
    "usunięcie skorelowanych atrybutów tj. usunięcie atrybutów 'unique_item_views' oraz 'item_views' i zachowanie atrybutu 'duration'\n",
    "Dla każdego z powyższych modeli obliczyliśmy miarę Accuracy oraz F1-score. Pierwsza wartość opisuje liczbę trafnych przewidywań do wszystkich próbek. Natomiast F1-score jest średnią harmoniczną z recall oraz precision.\n",
    "\n",
    "Zdecydowaliśmy się jednak, że naszą analityczną miarą sukcesu będzie F1-score, ponieważ nasze zadanie biznesowe polega na \"klasyfikacji sesji, które zakończą się zakupem co umożliwi konsultantom szybsze rozwiązywanie problemów\". Z tego można wywnioskować, że zależy nam na tym aby:\n",
    "\n",
    "liczba przypadków nieprawidłowo sklasyfikowanych jako sesje niekupujące była jak najmniejsza, dzięki czemu nie pomijamy sesji kupujących, które będą mogły wymagać interwencji konsultantów\n",
    "liczba przypadków sklasyfikowanych niepoprawnie jako sesja kupująca była jak najmniejsza dzięki czemu oszczędzamy czas konsultantów\n",
    "Z tego wynika, że bardziej zależy nam nam na False Negative'ach oraz False Positive'ach niż na True Positive'ach oraz True Negative'ach\n",
    "\n",
    "Analizując wyniki, możemy zauważyć że model Random Forest ma większą miarę F1 od regresji logistycznej zarówno w przypadku zachowania skorelowanych atrybutów jak i w przypadku usnięcia ich.\n",
    "W obu przypadkach postanowiliśmy usunąć atrybuty skorelowane tj. 'unique_item_views' oraz 'item_views' z racji na wartości F1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
